{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Personalized Music Recommender using Ensemble Methods"
      ],
      "metadata": {
        "id": "KUWFnsQNJXH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "qpnrvBzA1xW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xmkztkZP1rOh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import zipfile\n",
        "import os\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import sklearn as skl\n",
        "#import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, LabelBinarizer, StandardScaler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "!pip install graphviz\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "wI4p8d-vJm0x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjHBTeR-1rOi"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdRpRHVw1rOi"
      },
      "outputs": [],
      "source": [
        "!wget https://os.unil.cloud.switch.ch/fma/fma_metadata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0C8unry1rOj"
      },
      "outputs": [],
      "source": [
        "!unzip fma_metadata.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load(filepath):\n",
        "\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    if 'features' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
        "\n",
        "    if 'echonest' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
        "\n",
        "    if 'genres' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0)\n",
        "\n",
        "    if 'tracks' in filename:\n",
        "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
        "\n",
        "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
        "                   ('track', 'genres'), ('track', 'genres_all')]\n",
        "        for column in COLUMNS:\n",
        "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
        "\n",
        "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
        "                   ('album', 'date_created'), ('album', 'date_released'),\n",
        "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
        "                   ('artist', 'active_year_end')]\n",
        "        for column in COLUMNS:\n",
        "            tracks[column] = pd.to_datetime(tracks[column])\n",
        "\n",
        "        SUBSETS = ('small', 'medium', 'large')\n",
        "        try:\n",
        "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
        "                    'category', categories=SUBSETS, ordered=True)\n",
        "        except (ValueError, TypeError):\n",
        "            # the categories and ordered arguments were removed in pandas 0.25\n",
        "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
        "                     pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
        "\n",
        "        COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
        "                   ('album', 'type'), ('album', 'information'),\n",
        "                   ('artist', 'bio')]\n",
        "        for column in COLUMNS:\n",
        "            tracks[column] = tracks[column].astype('category')\n",
        "\n",
        "        return tracks"
      ],
      "metadata": {
        "id": "dUNMtRoH_CK9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracks = load('/content/fma_metadata/tracks.csv')\n",
        "genres = load('/content/fma_metadata/genres.csv')\n",
        "features = load('/content/fma_metadata/features.csv')\n",
        "echonest = load('/content/fma_metadata/echonest.csv')"
      ],
      "metadata": {
        "id": "KGxXT8l3_IRR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.testing.assert_array_equal(features.index, tracks.index)\n",
        "assert echonest.index.isin(tracks.index).all()\n",
        "\n",
        "tracks.shape, features.shape, echonest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdrzmow0DI-N",
        "outputId": "a1a1ccb8-9811-4130-a3ad-59b90639a04c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((106574, 52), (106574, 518), (13129, 249))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = tracks.index[tracks['set', 'subset'] <= 'medium']\n",
        "\n",
        "assert subset.isin(tracks.index).all()\n",
        "assert subset.isin(features.index).all()\n",
        "\n",
        "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
        "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
        "\n",
        "tracks = tracks.loc[subset]\n",
        "features_all = features.loc[subset]\n",
        "\n",
        "tracks.shape, features_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6flYQUBDROV",
        "outputId": "bddbf3d2-1e19-4c07-ae06-4b457d1b8b48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough Echonest features: (13129, 767)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 52), (25000, 518))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = tracks.index[tracks['set', 'split'] == 'training']\n",
        "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
        "test = tracks.index[tracks['set', 'split'] == 'test']\n",
        "\n",
        "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n",
        "\n",
        "genres = list(LabelEncoder().fit(tracks['track', 'genre_top']).classes_)\n",
        "print('Top genres ({}): {}'.format(len(genres), genres))\n",
        "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n",
        "print('All genres ({}): {}'.format(len(genres), genres))"
      ],
      "metadata": {
        "id": "PFtJk274DZI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46e7f66-dd02-453d-8238-74cb68cd2f6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19922 training examples, 2505 validation examples, 2573 testing examples\n",
            "Top genres (16): ['Blues', 'Classical', 'Country', 'Easy Listening', 'Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Jazz', 'Old-Time / Historic', 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
            "All genres (151): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 30, 31, 32, 33, 36, 37, 38, 41, 42, 43, 45, 46, 47, 49, 53, 58, 63, 64, 65, 66, 70, 71, 74, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 97, 98, 100, 101, 102, 103, 107, 109, 111, 113, 117, 118, 125, 130, 137, 138, 166, 167, 169, 171, 172, 174, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 214, 224, 232, 236, 240, 247, 250, 267, 286, 296, 297, 311, 314, 322, 337, 359, 360, 361, 362, 374, 378, 400, 401, 404, 428, 439, 440, 441, 442, 443, 456, 468, 491, 495, 502, 504, 514, 524, 538, 539, 542, 580, 602, 619, 651, 659, 695, 741, 763, 808, 810, 811, 906, 1032, 1060, 1193, 1235]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFVrwBkV-uZD",
        "outputId": "e45ed476-1d71-451c-bde6-ddf9d5076eb3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 52)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracks['track', 'genre_top']"
      ],
      "metadata": {
        "id": "i3sWgeLJBJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTbLkEUk1rOj"
      },
      "outputs": [],
      "source": [
        "!wget https://os.unil.cloud.switch.ch/fma/fma_small.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0kK9Rkc1rOk"
      },
      "outputs": [],
      "source": [
        "def batch_unzip(zip_path, output_dir, batch_size=0.1):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        all_files = z.namelist()\n",
        "        total_files = len(all_files)\n",
        "        batch_count = int(batch_size * total_files)\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Extract files in batches\n",
        "        for i in range(0, total_files, batch_count):\n",
        "            # Extract a subset of files\n",
        "            subset = all_files[i:i+batch_count]\n",
        "            for file in subset:\n",
        "                z.extract(file, output_dir)\n",
        "            print(f\"Batch {int(i/batch_count) + 1}/{int(total_files/batch_count) + 1} extracted\")\n",
        "\n",
        "# Usage\n",
        "zip_path = '/content/fma_small.zip'\n",
        "output_dir = '/content'\n",
        "batch_unzip(zip_path, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to Spectogram"
      ],
      "metadata": {
        "id": "kJpzG1RV5bHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_spectrogram(audio_path, save_path, save_as_png=False):\n",
        "    try:\n",
        "        # Load the audio file with librosa\n",
        "        y, sr = librosa.load(audio_path, sr=None)  # Use the native sampling rate\n",
        "\n",
        "        # Generate a Mel-scaled power (energy-squared) spectrogram\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "\n",
        "        # Convert to log scale (dB)\n",
        "        log_S = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        if save_as_png:\n",
        "            # Plot the Spectrogram\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "            plt.title('Mel Spectrogram')\n",
        "            plt.colorbar(format='%+02.0f dB')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "        else:\n",
        "            # Save as a numpy array file if not saving as PNG\n",
        "            np.save(save_path, log_S)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {audio_path}: {e}\")\n",
        "\n",
        "\n",
        "def process_audio_directory(input_dir, output_dir, save_as_png=False):\n",
        "    # Create the output directory if it does not exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Recursively process each file in the directory\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.mp3'):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                # Structure the output path to maintain directory structure\n",
        "                relative_path = os.path.relpath(root, input_dir)\n",
        "                output_subdir = os.path.join(output_dir, relative_path)\n",
        "                if not os.path.exists(output_subdir):\n",
        "                    os.makedirs(output_subdir)\n",
        "\n",
        "                output_name = filename.replace('.mp3', '.npy' if not save_as_png else '.png')\n",
        "                output_path = os.path.join(output_subdir, output_name)\n",
        "                convert_to_spectrogram(file_path, output_path, save_as_png=save_as_png)\n",
        "                print(f\"Processed and saved: {output_path}\")"
      ],
      "metadata": {
        "id": "h9dvd3BK5Zs8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Spectrograms as Numpy Arrays for carry CNN Model\n",
        "input_dir = '/content/fma_small/'  # Adjust the path to your actual directory structure\n",
        "output_dir = '/content/spectrograms_npy'  # Define the output directory for spectrograms\n",
        "process_audio_directory(input_dir, output_dir, save_as_png=False)"
      ],
      "metadata": {
        "id": "9AnRt8BjvMpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN model"
      ],
      "metadata": {
        "id": "XcrEFQTk5tA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter needed columns and drop NaNs\n",
        "genre_labels = tracks['track', 'genre_top'].dropna()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "def load_spectrograms_and_labels(spectrogram_dir, genre_labels, max_pad_len=174):\n",
        "    X = []\n",
        "    y = []\n",
        "    for root, dirs, files in os.walk(spectrogram_dir):  # Recursively walk through all directories\n",
        "        for filename in files:\n",
        "            if filename.endswith('.npy'):  # Ensure we are only processing .npy files\n",
        "                track_id = int(filename.split('.')[0])\n",
        "                if track_id in genre_labels.index:\n",
        "                    genre = genre_labels.loc[track_id]\n",
        "                    spectrogram_path = os.path.join(root, filename)\n",
        "                    spectrogram = np.load(spectrogram_path)\n",
        "\n",
        "                    # Pad or trim the length of the spectrogram\n",
        "                    pad_width = max_pad_len - spectrogram.shape[1]\n",
        "                    if pad_width > 0:\n",
        "                        spectrogram = np.pad(spectrogram, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "                    else:\n",
        "                        spectrogram = spectrogram[:, :max_pad_len]\n",
        "\n",
        "                    X.append(spectrogram)\n",
        "                    y.append(genre)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "spectrogram_dir = '/content/spectrograms_npy'\n",
        "X, y = load_spectrograms_and_labels(spectrogram_dir, genre_labels)\n",
        "\n",
        "# Adding a channel dimension to handle CNN input requirements\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "F4cWLRAQWtYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X has shape (num_samples, height, width)\n",
        "\n",
        "input_shape = X_train[0].shape  # Assuming X_train is correctly shaped (samples, height, width, channels)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(2, 2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "djPC_IJOXrtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "Ec5du81JFO0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "wmlHPwPDYZSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "oPr6Ymd_YgBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking"
      ],
      "metadata": {
        "id": "oHs1ZWO_JJTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import sklearn as skl\n",
        "#import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#!wget https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
        "\n",
        "!wget https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\n",
        "!unzip fma_metadata.zip\n",
        "# move to Hadoop Namenode\n",
        "#!hadoop fs -put enwiki-latest-pages-articles1.xml /\n",
        "\n",
        "\n",
        "!hadoop fs -put fma_metadata/* /\n",
        "\n",
        "#hadoop fs -put /local/path/to/fma_metadata/tracks.csv /hdfs/target/path/\n",
        "# adjust to reflect the cluster name and Hadoop masternode (IP port) of your cluster\n",
        "tracks = \"hdfs://st446-w09-cluster-m:8020/tracks.csv\"\n",
        "\n",
        "genres = \"hdfs://st446-w09-cluster-m:8020/genres.csv\"\n",
        "features = \"hdfs://st446-w09-cluster-m:8020/features.csv\"\n",
        "echonest = \"hdfs://st446-w09-cluster-m:8020/echonest.csv\"\n",
        "df1 = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"multiline\", \"true\") \\\n",
        "    .load(tracks)\n",
        "\n",
        "df1.limit(10).toPandas()\n",
        "\n",
        "featuredf = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"multiline\", \"true\") \\\n",
        "    .load(features)\n",
        "\n",
        "featuredf.limit(10).toPandas()\n",
        "\n",
        "#small track (df1) dataset\n",
        "\n",
        "small_df = df1.filter(col('set32') == 'small')\n",
        "small_df.count()\n",
        "\n",
        "#small feature dataset(plus track)\n",
        "\n",
        "joined_df = df1.join(featuredf, df1._c0 == featuredf.feature)\n",
        "small_featuredf=joined_df.filter(col('set32') == 'small')\n",
        "#small_featuredf.printSchema()\n",
        "mfcc_columns = [column for column in small_featuredf.columns if \"mfcc\" in column]\n",
        "#track40 is genre top\n",
        "\n",
        "#split the dataset\n",
        "training=small_featuredf.filter(col('set31') == 'training').select(['track40'] + mfcc_columns)\n",
        "test=small_featuredf.filter(col('set31') == 'test').select(['track40'] + mfcc_columns)\n",
        "validation=small_featuredf.filter(col('set31') == 'validation').select(['track40'] + mfcc_columns)\n",
        "# convert datatype to float\n",
        "for feature in mfcc_columns:\n",
        "    training=training.withColumn(feature, col(feature).cast('float'))\n",
        "    test=test.withColumn(feature, col(feature).cast('float'))\n",
        "    validation=validation.withColumn(feature, col(feature).cast('float'))\n",
        "validation.printSchema()\n",
        "\n",
        "genre classification (from features -mfcc) using Randomforest\n",
        "test.printSchema()\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier, OneVsRest, DecisionTreeClassifier, LogisticRegression, LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Convert label to number\n",
        "stringIndexer = StringIndexer(inputCol=\"track40\", outputCol=\"indexed\")\n",
        "\n",
        "# feature columns into a vector  #usetesthere which should be train\n",
        "featuresCreator = VectorAssembler(inputCols=test.columns[1:], outputCol=\"features\")\n",
        "\n",
        "# Define and fit the model\n",
        "rf = RandomForestClassifier(labelCol=\"indexed\", featuresCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[stringIndexer, featuresCreator, rf])\n",
        "\n",
        "#pipeline model\n",
        "model = pipeline.fit(training)\n",
        "#make predictions\n",
        "predictions=model.transform(test)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexed\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#final_df.printSchema()\n",
        "\n",
        "test.printSchema()\n",
        "\n",
        "#different classifiers\n",
        "\n",
        "#rf = rf = RandomForestClassifier(labelCol=\"indexed\", featuresCol=\"features\")\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexed\", featuresCol=\"features\")\n",
        "#lr = LogisticRegression()\n",
        "\n",
        "# base pipelines\n",
        "#rf_pipeline = Pipeline(stages=[stringIndexer,featuresCreator,\n",
        "    #rf\n",
        "#])\n",
        "\n",
        "\n",
        "dt_pipeline = Pipeline(stages=[stringIndexer,featuresCreator,\n",
        "    dt\n",
        "])\n",
        "\n",
        "\n",
        "# fit the base models\n",
        "\n",
        "#rf_model = rf_pipeline.fit(training)\n",
        "dt_model = dt_pipeline.fit(training)\n",
        "\n",
        "# predictions (should be trained on validationtest)\n",
        "#rf_predict = rf_model.transform(validation)    in last part, rf prediction is predictions\n",
        "\n",
        "dt_predict = dt_model.transform(test)\n",
        "#dt_predict.printSchema()\n",
        "dt_predict.head()\n",
        "\n",
        "choose probability column as meta-feature\n",
        "# create new features for stacking\n",
        "\n",
        "#add id\n",
        "#reference https://stackoverflow.com/questions/43406887/spark-dataframe-how-to-add-a-index-column-aka-distributed-data-index\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "predictions=predictions.withColumn(\"id\", monotonically_increasing_id())\n",
        "dt_predict=dt_predict.withColumn(\"id\", monotonically_increasing_id())\n",
        "validation=validation.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "# rename probability column\n",
        "predictions = predictions.withColumnRenamed('probability', 'rf_probability')\n",
        "dt_predict = dt_predict.withColumnRenamed('probability', 'dt_probability')\n",
        "\n",
        "#join the prediction to the validation dataset\n",
        "newfeature = validation \\\n",
        "    .join(predictions.select('id', 'rf_probability'), on='id') \\\n",
        "    .join(dt_predict.select('id', 'dt_probability'), on='id')\n",
        "# Assemble new features for meta-model\n",
        "stacking_assembler = VectorAssembler(inputCols=['rf_prob', 'gbt_prob'], outputCol='stacking_features')\n",
        "\n",
        "# Meta-model pipeline : use logistic regression\n",
        "meta_pipeline = Pipeline(stages=[\n",
        "    stacking_assembler,lr\n",
        "])\n",
        "\n",
        "# Fit the meta-model\n",
        "meta_model = meta_pipeline.fit(new_feature)\n",
        "# meta-model make predictions\n",
        "final_predictions = meta_model.transform(new_features)"
      ],
      "metadata": {
        "id": "ymu3KsMEJKdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df1 is already loaded and contains genre information\n",
        "genre_counts = df1.groupBy('genre_top').count().toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='count', y='genre_top', data=genre_counts)\n",
        "plt.title('Distribution of Genres')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Genre')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2IiOzEp7NVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Feature Engineering\n",
        "\n",
        "from pyspark.ml.feature import PCA, VectorAssembler\n",
        "\n",
        "# Assume 'featuredf' contains all features including MFCCs\n",
        "feature_columns = [c for c in featuredf.columns if \"mfcc\" in c]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "feature_vector = assembler.transform(featuredf)\n",
        "\n",
        "# PCA to reduce dimensions\n",
        "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "pca_model = pca.fit(feature_vector)\n",
        "pca_result = pca_model.transform(feature_vector)\n",
        "\n",
        "# Adding reduced dimensions back to original dataframe\n",
        "df1 = df1.join(pca_result.select('track_id', 'pcaFeatures'), on='track_id')"
      ],
      "metadata": {
        "id": "cuOHo7oyNXDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Pipeline already defined as 'pipeline'\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20, 50]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"indexed\", predictionCol=\"prediction\"),\n",
        "                          numFolds=5)  # 5-fold cross-validation\n",
        "\n",
        "# Train the model using CrossValidator\n",
        "cvModel = crossval.fit(training)\n"
      ],
      "metadata": {
        "id": "Qwo31W_DNeAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking Implementation\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Assembling new features from base model predictions\n",
        "stacking_assembler = VectorAssembler(inputCols=['rf_probability', 'dt_probability'], outputCol='stacking_features')\n",
        "\n",
        "# Meta-model: Logistic Regression\n",
        "lr = LogisticRegression(featuresCol='stacking_features', labelCol='indexed')\n",
        "stacking_pipeline = Pipeline(stages=[stacking_assembler, lr])\n",
        "\n",
        "# Training meta-model on predictions\n",
        "stacking_model = stacking_pipeline.fit(newfeature)  # Ensure 'newfeature' is prepared correctly\n",
        "\n",
        "# Making final predictions\n",
        "final_predictions = stacking_model.transform(new_features)  # 'new_features' should be the dataset prepared for the meta-model"
      ],
      "metadata": {
        "id": "ESuRnHdBNi6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging"
      ],
      "metadata": {
        "id": "oAUPKBNDJLKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAXnyR4BJMId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting"
      ],
      "metadata": {
        "id": "gOsviE8gJMmP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejMOHyQ5JNeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}